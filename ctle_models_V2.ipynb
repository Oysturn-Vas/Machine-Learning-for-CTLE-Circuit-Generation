{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import (\n",
    "    RobustScaler,\n",
    "    OneHotEncoder,\n",
    "    StandardScaler,\n",
    "    PowerTransformer,\n",
    ")\n",
    "from sklearn.multioutput import MultiOutputRegressor, RegressorChain\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from scipy.optimize import minimize\n",
    "import warnings\n",
    "import joblib\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import time\n",
    "import matplotlib\n",
    "\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except ImportError:\n",
    "    def tqdm(iterable, **kwargs):\n",
    "        return iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"default\")\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory = \"ctle_circuit_generation_with_DNN_models_V2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IQROutlierCapper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, lower_bound_mult=1.5, upper_bound_mult=1.5):\n",
    "        self.lower_bound_mult = lower_bound_mult\n",
    "        self.upper_bound_mult = upper_bound_mult\n",
    "        self.lower_bounds_ = {}\n",
    "        self.upper_bounds_ = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = pd.DataFrame(X) if not isinstance(X, pd.DataFrame) else X\n",
    "        for col in X.columns:\n",
    "            if pd.api.types.is_numeric_dtype(X[col]) and not X[col].isnull().all():\n",
    "                Q1 = X[col].quantile(0.25)\n",
    "                Q3 = X[col].quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                self.lower_bounds_[col] = Q1 - (IQR * self.lower_bound_mult)\n",
    "                self.upper_bounds_[col] = Q3 + (IQR * self.upper_bound_mult)\n",
    "            else:\n",
    "                self.lower_bounds_[col] = -np.inf\n",
    "                self.upper_bounds_[col] = np.inf\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = pd.DataFrame(X) if not isinstance(X, pd.DataFrame) else X\n",
    "        X_copy = X.copy()\n",
    "        for col in X_copy.columns:\n",
    "            if (\n",
    "                col in self.lower_bounds_\n",
    "                and pd.api.types.is_numeric_dtype(X_copy[col])\n",
    "                and self.lower_bounds_[col] != -np.inf\n",
    "                and self.upper_bounds_[col] != np.inf\n",
    "            ):\n",
    "                X_copy[col] = X_copy[col].clip(\n",
    "                    lower=self.lower_bounds_[col], upper=self.upper_bounds_[col]\n",
    "                )\n",
    "        return X_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"DataV2.csv\"\n",
    "df = pd.read_csv(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_targets = [\n",
    "    \"stage 1 3.5G attenuation\",\n",
    "    \"stage 2 3.5G attenuation\",\n",
    "    \"stage 1 7G attenuation\",\n",
    "    \"stage 2 7G attenuation\",\n",
    "    \"stage 1 14G attenuation\",\n",
    "    \"stage 2 14G attenuation\",\n",
    "    \"stage 1 28G attenuation\",\n",
    "    \"stage 2 28G attenuation\",\n",
    "    \"eye_maxHeight Vout_1 7G\",\n",
    "    \"eye_maxWidth Vout_1 7G\",\n",
    "    \"eye_maxHeight Vout_2 7G\",\n",
    "    \"eye_maxWidth Vout_2 7G\",\n",
    "    \"eye_maxHeight Vout_1 14G\",\n",
    "    \"eye_maxWidth Vout_1 14G\",\n",
    "    \"eye_maxHeight Vout_2 14G\",\n",
    "    \"eye_maxWidth Vout_2 14G\",\n",
    "    \"eye_maxHeight Vout_1 28G\",\n",
    "    \"eye_maxWidth Vout_1 28G\",\n",
    "    \"eye_maxHeight Vout_2 28G\",\n",
    "    \"eye_maxWidth Vout_2 28G\",\n",
    "    \"eye_maxHeight Vout_1 56G\",\n",
    "    \"eye_maxWidth Vout_1 56G\",\n",
    "    \"eye_maxHeight Vout_2 56G\",\n",
    "    \"eye_maxWidth Vout_2 56G\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_targets = [col for col in all_targets if col in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_targets = []\n",
    "sparse_targets = []\n",
    "constant_targets = []\n",
    "problematic_targets = []\n",
    "target_analysis = {}\n",
    "\n",
    "for col in available_targets:\n",
    "    values = df[col]\n",
    "    zero_pct = (values == 0).sum() / len(values) * 100\n",
    "    variance = values.var()\n",
    "    cv = values.std() / abs(values.mean()) if values.mean() != 0 else float(\"inf\")\n",
    "    unique_values = values.nunique()\n",
    "\n",
    "    target_analysis[col] = {\n",
    "        \"zero_pct\": zero_pct,\n",
    "        \"variance\": variance,\n",
    "        \"cv\": cv,\n",
    "        \"mean\": values.mean(),\n",
    "        \"std\": values.std(),\n",
    "        \"unique_values\": unique_values,\n",
    "        \"min\": values.min(),\n",
    "        \"max\": values.max(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = available_targets\n",
    "feature_names = [col for col in df.columns if col not in all_targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[feature_names].copy()\n",
    "y_raw = df[target_names].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_processed = y_raw.copy()\n",
    "constant_noise_std = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle constant targets by adding small noise\n",
    "for col in constant_targets:\n",
    "    if col in y_processed.columns:\n",
    "        # Add small random noise to constant targets to make them learnable\n",
    "        noise = np.random.normal(0, constant_noise_std, len(y_processed))\n",
    "        y_processed[col] = y_processed[col] + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle sparse targets with log transformation and offset\n",
    "sparse_offset = 1e-3\n",
    "for col in sparse_targets:\n",
    "    if col in y_processed.columns:\n",
    "        # Apply log transformation with offset for sparse targets\n",
    "        min_val = y_processed[col].min()\n",
    "        if min_val <= 0:\n",
    "            # Shift to positive values\n",
    "            y_processed[col] = y_processed[col] - min_val + sparse_offset\n",
    "\n",
    "        # Apply log1p transformation (log(1+x)) for sparse data\n",
    "        y_processed[col] = np.log1p(np.abs(y_processed[col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Yeo-Johnson transformation for better normality\n",
    "power_transformer = PowerTransformer(method=\"yeo-johnson\", standardize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only apply to targets with sufficient variance\n",
    "transformable_targets = [\n",
    "    col\n",
    "    for col in target_names\n",
    "    if col not in constant_targets and y_processed[col].var() > 1e-8\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if transformable_targets:\n",
    "    y_transformed = y_processed.copy()\n",
    "\n",
    "    # Transform each target individually to handle different distributions\n",
    "    for col in transformable_targets:\n",
    "        try:\n",
    "            col_data = y_processed[col].values.reshape(-1, 1)\n",
    "            transformed_data = power_transformer.fit_transform(col_data)\n",
    "            y_transformed[col] = transformed_data.flatten()\n",
    "        except:\n",
    "            # If transformation fails, use robust scaling\n",
    "            scaler = RobustScaler()\n",
    "            col_data = y_processed[col].values.reshape(-1, 1)\n",
    "            scaled_data = scaler.fit_transform(col_data)\n",
    "            y_transformed[col] = scaled_data.flatten()\n",
    "\n",
    "    y = y_transformed\n",
    "else:\n",
    "    y = y_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store preprocessing info\n",
    "target_preprocessing_info = {\n",
    "    \"constant_targets\": constant_targets,\n",
    "    \"sparse_targets\": sparse_targets,\n",
    "    \"good_targets\": good_targets,\n",
    "    \"problematic_targets\": problematic_targets,\n",
    "    \"transformable_targets\": (\n",
    "        transformable_targets if \"transformable_targets\" in locals() else []\n",
    "    ),\n",
    "    \"constant_noise_std\": constant_noise_std,\n",
    "    \"sparse_offset\": sparse_offset,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numerical and categorical features\n",
    "numerical_features = [\n",
    "    col\n",
    "    for col in feature_names\n",
    "    if col not in [\"Stage 1 Region\", \"Hard Constraint of neg2 on 0.1G Status\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = [\n",
    "    col\n",
    "    for col in feature_names\n",
    "    if col in [\"Stage 1 Region\", \"Hard Constraint of neg2 on 0.1G Status\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical features: 13\n",
      "Categorical features: 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Numerical features: {len(numerical_features)}\")\n",
    "print(f\"Categorical features: {len(categorical_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocessing pipelines\n",
    "numerical_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"outlier_capper\", IQROutlierCapper()),\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", RobustScaler()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "categorical_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Combine pipelines\n",
    "if categorical_features:\n",
    "    preprocessor = ColumnTransformer(\n",
    "        [\n",
    "            (\"num\", numerical_pipeline, numerical_features),\n",
    "            (\"cat\", categorical_pipeline, categorical_features),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "    )\n",
    "else:\n",
    "    preprocessor = numerical_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REORDER TARGETS FOR CHAINED PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage1_targets = [col for col in target_names if \"stage 1\" in col.lower()]\n",
    "stage2_targets = [col for col in target_names if \"stage 2\" in col.lower()]\n",
    "other_targets = [\n",
    "    col for col in target_names if col not in stage1_targets + stage2_targets\n",
    "]\n",
    "\n",
    "# New order: Stage 1 → Stage 2 → Others\n",
    "reordered_target_names = stage1_targets + stage2_targets + other_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder the target DataFrame and update target_names\n",
    "y = y[reordered_target_names]\n",
    "target_names = reordered_target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN-TEST SPLIT AND PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"Hard Constraint of neg2 on 0.1G Status\" in X.columns:\n",
    "    stratify = X[\"Hard Constraint of neg2 on 0.1G Status\"]\n",
    "else:\n",
    "    stratify = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=stratify\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "model_performance = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models optimized for ALL target types with CHAINED PREDICTIONS\n",
    "models_config = {\n",
    "    \"xgboost_robust\": {\n",
    "        \"model\": RegressorChain(\n",
    "            base_estimator=XGBRegressor(\n",
    "                n_estimators=300,\n",
    "                max_depth=8,\n",
    "                learning_rate=0.05,  # Lower learning rate for stability\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                reg_alpha=0.5,  # Higher regularization for sparse targets\n",
    "                reg_lambda=2.0,\n",
    "                gamma=0.1,  # Minimum split loss for sparse data\n",
    "                min_child_weight=3,  # Higher minimum child weight\n",
    "                random_state=42,\n",
    "                n_jobs=-1,\n",
    "            ),\n",
    "            order=None,  # Use default order (Stage 1 → Stage 2 → Others)\n",
    "            random_state=42,\n",
    "        ),\n",
    "        \"name\": \"XGBoost Robust (Chained)\",\n",
    "    },\n",
    "    \"lightgbm_robust\": {\n",
    "        \"model\": RegressorChain(\n",
    "            base_estimator=LGBMRegressor(\n",
    "                n_estimators=300,\n",
    "                max_depth=8,\n",
    "                learning_rate=0.05,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                reg_alpha=0.5,\n",
    "                reg_lambda=2.0,\n",
    "                min_child_samples=10,  # Higher minimum samples for stability\n",
    "                min_split_gain=0.1,\n",
    "                random_state=42,\n",
    "                n_jobs=-1,\n",
    "                verbose=-1,\n",
    "            ),\n",
    "            order=None,\n",
    "            random_state=42,\n",
    "        ),\n",
    "        \"name\": \"LightGBM Robust (Chained)\",\n",
    "    },\n",
    "    \"random_forest_robust\": {\n",
    "        \"model\": RegressorChain(\n",
    "            base_estimator=RandomForestRegressor(\n",
    "                n_estimators=200,\n",
    "                max_depth=12,\n",
    "                min_samples_split=10,  # Higher for sparse data\n",
    "                min_samples_leaf=5,\n",
    "                max_features=\"sqrt\",  # Reduce overfitting\n",
    "                random_state=42,\n",
    "                n_jobs=-1,\n",
    "            ),\n",
    "            order=None,\n",
    "            random_state=42,\n",
    "        ),\n",
    "        \"name\": \"Random Forest Robust (Chained)\",\n",
    "    },\n",
    "    \"mlp_robust\": {\n",
    "        \"model\": RegressorChain(\n",
    "            base_estimator=MLPRegressor(\n",
    "                hidden_layer_sizes=(128, 64, 32),  # Larger network for complex patterns\n",
    "                activation=\"relu\",\n",
    "                solver=\"adam\",\n",
    "                alpha=0.1,  # Higher regularization\n",
    "                learning_rate_init=0.001,\n",
    "                max_iter=500,\n",
    "                early_stopping=True,\n",
    "                validation_fraction=0.2,\n",
    "                n_iter_no_change=30,\n",
    "                random_state=42,\n",
    "            ),\n",
    "            order=None,\n",
    "            random_state=42,\n",
    "        ),\n",
    "        \"name\": \"Neural Network Robust (Chained)\",\n",
    "    },\n",
    "    \"gradient_boosting_robust\": {\n",
    "        \"model\": RegressorChain(\n",
    "            base_estimator=GradientBoostingRegressor(\n",
    "                n_estimators=200,\n",
    "                max_depth=8,\n",
    "                learning_rate=0.05,\n",
    "                subsample=0.8,\n",
    "                min_samples_split=10,\n",
    "                min_samples_leaf=5,\n",
    "                random_state=42,\n",
    "            ),\n",
    "            order=None,\n",
    "            random_state=42,\n",
    "        ),\n",
    "        \"name\": \"Gradient Boosting Robust (Chained)\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Training XGBoost Robust (Chained)...\n",
      "XGBoost Robust (Chained): R² = 0.5102, RMSE = 0.3917, Time = 5.2s\n",
      "Performance breakdown: 10 excellent, 2 good, 9 negative\n",
      "Challenging targets avg R²: 0.5102 (24 targets)\n",
      "\n",
      "  Training LightGBM Robust (Chained)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Program Files\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Program Files\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Program Files\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Program Files\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Program Files\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Program Files\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Program Files\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Program Files\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Program Files\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Program Files\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Program Files\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Program Files\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Program Files\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Program Files\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Program Files\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Program Files\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Program Files\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Program Files\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Program Files\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Program Files\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Program Files\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Program Files\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Program Files\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Program Files\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Program Files\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Program Files\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Program Files\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Program Files\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Program Files\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Program Files\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Program Files\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Program Files\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Program Files\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Program Files\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Program Files\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Program Files\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Program Files\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Program Files\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Program Files\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Program Files\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Program Files\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Program Files\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Program Files\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Program Files\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Program Files\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Program Files\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Program Files\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM Robust (Chained): R² = 0.5110, RMSE = 0.3907, Time = 4.7s\n",
      "Performance breakdown: 10 excellent, 2 good, 9 negative\n",
      "Challenging targets avg R²: 0.5110 (24 targets)\n",
      "\n",
      "  Training Random Forest Robust (Chained)...\n",
      "Random Forest Robust (Chained): R² = 0.5095, RMSE = 0.3927, Time = 9.1s\n",
      "Performance breakdown: 10 excellent, 2 good, 8 negative\n",
      "Challenging targets avg R²: 0.5095 (24 targets)\n",
      "\n",
      "  Training Neural Network Robust (Chained)...\n",
      "Neural Network Robust (Chained): R² = -3669896129105.1060, RMSE = 0.2272, Time = 187.4s\n",
      "Performance breakdown: 12 excellent, 2 good, 8 negative\n",
      "Challenging targets avg R²: -3669896129105.1060 (24 targets)\n",
      "\n",
      "  Training Gradient Boosting Robust (Chained)...\n",
      "Gradient Boosting Robust (Chained): R² = 0.5068, RMSE = 0.3959, Time = 200.9s\n",
      "Performance breakdown: 10 excellent, 2 good, 9 negative\n",
      "Challenging targets avg R²: 0.5068 (24 targets)\n"
     ]
    }
   ],
   "source": [
    "for model_key, config in models_config.items():\n",
    "    print(f\"\\n  Training {config['name']}...\")\n",
    "    start_model_time = time.time()\n",
    "\n",
    "    # Train model\n",
    "    model = config[\"model\"]\n",
    "    model.fit(X_train_processed, y_train)\n",
    "\n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test_processed)\n",
    "\n",
    "    # Evaluate\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "    # Enhanced evaluation for different target types\n",
    "    individual_r2 = []\n",
    "    individual_mae = []\n",
    "    negative_r2_count = 0\n",
    "    good_r2_count = 0\n",
    "    excellent_r2_count = 0\n",
    "\n",
    "    target_performance = {}\n",
    "\n",
    "    for i in range(y_test.shape[1]):\n",
    "        target_name = target_names[i]\n",
    "        target_r2 = r2_score(y_test.iloc[:, i], y_pred[:, i])\n",
    "        target_mae = mean_absolute_error(y_test.iloc[:, i], y_pred[:, i])\n",
    "\n",
    "        individual_r2.append(target_r2)\n",
    "        individual_mae.append(target_mae)\n",
    "\n",
    "        # Categorize performance\n",
    "        if target_r2 < 0:\n",
    "            negative_r2_count += 1\n",
    "            performance_category = \"negative\"\n",
    "        elif target_r2 > 0.9:\n",
    "            excellent_r2_count += 1\n",
    "            performance_category = \"excellent\"\n",
    "        elif target_r2 > 0.7:\n",
    "            good_r2_count += 1\n",
    "            performance_category = \"good\"\n",
    "        else:\n",
    "            performance_category = \"fair\"\n",
    "\n",
    "        target_performance[target_name] = {\n",
    "            \"r2\": target_r2,\n",
    "            \"mae\": target_mae,\n",
    "            \"category\": performance_category,\n",
    "            \"target_type\": (\n",
    "                \"constant\"\n",
    "                if target_name in constant_targets\n",
    "                else (\n",
    "                    \"sparse\"\n",
    "                    if target_name in sparse_targets\n",
    "                    else \"good\" if target_name in good_targets else \"challenging\"\n",
    "                )\n",
    "            ),\n",
    "        }\n",
    "\n",
    "    training_time = time.time() - start_model_time\n",
    "\n",
    "    # Store comprehensive results\n",
    "    models[model_key] = model\n",
    "    model_performance[model_key] = {\n",
    "        \"name\": config[\"name\"],\n",
    "        \"r2\": r2,\n",
    "        \"rmse\": rmse,\n",
    "        \"mae\": mae,\n",
    "        \"individual_r2\": individual_r2,\n",
    "        \"individual_mae\": individual_mae,\n",
    "        \"target_performance\": target_performance,\n",
    "        \"negative_r2_count\": negative_r2_count,\n",
    "        \"good_r2_count\": good_r2_count,\n",
    "        \"excellent_r2_count\": excellent_r2_count,\n",
    "        \"training_time\": training_time,\n",
    "    }\n",
    "\n",
    "    print(\n",
    "        f\"{config['name']}: R² = {r2:.4f}, RMSE = {rmse:.4f}, Time = {training_time:.1f}s\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Performance breakdown: {excellent_r2_count} excellent, {good_r2_count} good, {negative_r2_count} negative\"\n",
    "    )\n",
    "\n",
    "    # Show performance by target type\n",
    "    type_performance = {}\n",
    "    for target_type in [\"good\", \"sparse\", \"constant\", \"challenging\"]:\n",
    "        type_r2_values = [\n",
    "            perf[\"r2\"]\n",
    "            for perf in target_performance.values()\n",
    "            if perf[\"target_type\"] == target_type\n",
    "        ]\n",
    "        if type_r2_values:\n",
    "            avg_r2 = np.mean(type_r2_values)\n",
    "            type_performance[target_type] = avg_r2\n",
    "            print(\n",
    "                f\"{target_type.capitalize()} targets avg R²: {avg_r2:.4f} ({len(type_r2_values)} targets)\"\n",
    "            )\n",
    "\n",
    "    model_performance[model_key][\"type_performance\"] = type_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "XGBoost Robust (Chained):\n",
      "Overall R²: 0.5102\n",
      "Excellent targets: 10/24 (41.7%)\n",
      "Good targets: 2/24 (8.3%)\n",
      "Negative R²: 9/24 (37.5%)\n",
      "Composite score: 0.4083\n",
      "\n",
      "LightGBM Robust (Chained):\n",
      "Overall R²: 0.5110\n",
      "Excellent targets: 10/24 (41.7%)\n",
      "Good targets: 2/24 (8.3%)\n",
      "Negative R²: 9/24 (37.5%)\n",
      "Composite score: 0.4086\n",
      "\n",
      "Random Forest Robust (Chained):\n",
      "Overall R²: 0.5095\n",
      "Excellent targets: 10/24 (41.7%)\n",
      "Good targets: 2/24 (8.3%)\n",
      "Negative R²: 8/24 (33.3%)\n",
      "Composite score: 0.4121\n",
      "\n",
      "Neural Network Robust (Chained):\n",
      "Overall R²: -3669896129105.1060\n",
      "Excellent targets: 12/24 (50.0%)\n",
      "Good targets: 2/24 (8.3%)\n",
      "Negative R²: 8/24 (33.3%)\n",
      "Composite score: -1467958451641.8093\n",
      "\n",
      "Gradient Boosting Robust (Chained):\n",
      "Overall R²: 0.5068\n",
      "Excellent targets: 10/24 (41.7%)\n",
      "Good targets: 2/24 (8.3%)\n",
      "Negative R²: 9/24 (37.5%)\n",
      "Composite score: 0.4069\n"
     ]
    }
   ],
   "source": [
    "model_scores = {}\n",
    "for model_key, performance in model_performance.items():\n",
    "    # Composite score considering multiple factors\n",
    "    base_r2 = performance[\"r2\"]\n",
    "    excellent_ratio = performance[\"excellent_r2_count\"] / len(target_names)\n",
    "    good_ratio = performance[\"good_r2_count\"] / len(target_names)\n",
    "    negative_penalty = performance[\"negative_r2_count\"] / len(target_names)\n",
    "\n",
    "    # Weighted composite score\n",
    "    composite_score = (\n",
    "        base_r2 * 0.4  # Overall R²\n",
    "        + excellent_ratio * 0.3  # Excellent targets bonus\n",
    "        + good_ratio * 0.2  # Good targets bonus\n",
    "        + (1 - negative_penalty) * 0.1  # Negative R² penalty\n",
    "    )\n",
    "\n",
    "    model_scores[model_key] = composite_score\n",
    "\n",
    "    print(f\"\\n{performance['name']}:\")\n",
    "    print(f\"Overall R²: {base_r2:.4f}\")\n",
    "    print(\n",
    "        f\"Excellent targets: {performance['excellent_r2_count']}/{len(target_names)} ({excellent_ratio:.1%})\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Good targets: {performance['good_r2_count']}/{len(target_names)} ({good_ratio:.1%})\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Negative R²: {performance['negative_r2_count']}/{len(target_names)} ({negative_penalty:.1%})\"\n",
    "    )\n",
    "    print(f\"Composite score: {composite_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_name = max(model_scores.keys(), key=lambda x: model_scores[x])\n",
    "best_composite_score = model_scores[best_model_name]\n",
    "\n",
    "best_model = models[best_model_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best model: Random Forest Robust (Chained)\n",
      "R² = 0.5095\n",
      "RMSE = 0.3927\n",
      "Negative R² count = 8\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nBest model: {model_performance[best_model_name]['name']}\")\n",
    "print(f\"R² = {model_performance[best_model_name]['r2']:.4f}\")\n",
    "print(f\"RMSE = {model_performance[best_model_name]['rmse']:.4f}\")\n",
    "print(\n",
    "    f\"Negative R² count = {model_performance[best_model_name]['negative_r2_count']}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILD REVERSE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for reverse modeling with ALL targets\n",
    "# Use only numerical circuit parameters for reverse prediction\n",
    "numerical_params = [\n",
    "    col\n",
    "    for col in feature_names\n",
    "    if col not in [\"Stage 1 Region\", \"Hard Constraint of neg2 on 0.1G Status\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reverse = df[numerical_params].copy()  # Circuit parameters to predict\n",
    "y_reverse = y.copy()  # Use preprocessed targets as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "X_reverse = X_reverse.fillna(X_reverse.median())\n",
    "y_reverse = y_reverse.fillna(y_reverse.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove extreme outliers that could destabilize reverse model\n",
    "for col in numerical_params:\n",
    "    Q1 = X_reverse[col].quantile(0.01)\n",
    "    Q99 = X_reverse[col].quantile(0.99)\n",
    "    X_reverse[col] = X_reverse[col].clip(lower=Q1, upper=Q99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split for reverse model\n",
    "X_rev_train, X_rev_test, y_rev_train, y_rev_test = train_test_split(\n",
    "    X_reverse, y_reverse, test_size=0.3, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reverse preprocessors\n",
    "reverse_X_scaler = RobustScaler()  # For circuit parameters\n",
    "reverse_y_scaler = RobustScaler()  # For performance targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess\n",
    "X_rev_train_scaled = reverse_X_scaler.fit_transform(X_rev_train)\n",
    "X_rev_test_scaled = reverse_X_scaler.transform(X_rev_test)\n",
    "y_rev_train_scaled = reverse_y_scaler.fit_transform(y_rev_train)\n",
    "y_rev_test_scaled = reverse_y_scaler.transform(y_rev_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # For RegressorChain, access the base_estimator attribute\n",
    "    best_forward_estimator = models[best_model_name].base_estimator\n",
    "except AttributeError:\n",
    "    # Fallback for other model types\n",
    "    try:\n",
    "        best_forward_estimator = models[best_model_name].estimators_[0]\n",
    "    except:\n",
    "        best_forward_estimator = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reverse model using the same architecture as the best forward model\n",
    "if best_forward_estimator is not None and best_model_name == \"xgboost_robust\":\n",
    "    # Use XGBoost with similar parameters but optimized for reverse modeling\n",
    "    reverse_base_model = XGBRegressor(\n",
    "        n_estimators=best_forward_estimator.n_estimators,\n",
    "        max_depth=min(\n",
    "            best_forward_estimator.max_depth + 2, 12\n",
    "        ),  # Slightly deeper for inverse complexity\n",
    "        learning_rate=max(\n",
    "            best_forward_estimator.learning_rate * 0.8, 0.01\n",
    "        ),  # Slightly lower learning rate\n",
    "        subsample=best_forward_estimator.subsample,\n",
    "        colsample_bytree=best_forward_estimator.colsample_bytree,\n",
    "        reg_alpha=best_forward_estimator.reg_alpha\n",
    "        * 1.5,  # Higher regularization for stability\n",
    "        reg_lambda=best_forward_estimator.reg_lambda * 1.5,\n",
    "        gamma=best_forward_estimator.gamma,\n",
    "        min_child_weight=best_forward_estimator.min_child_weight,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "elif best_forward_estimator is not None and best_model_name == \"lightgbm_robust\":\n",
    "    # Use LightGBM with similar parameters\n",
    "    reverse_base_model = LGBMRegressor(\n",
    "        n_estimators=best_forward_estimator.n_estimators,\n",
    "        max_depth=min(best_forward_estimator.max_depth + 2, 12),\n",
    "        learning_rate=max(best_forward_estimator.learning_rate * 0.8, 0.01),\n",
    "        subsample=best_forward_estimator.subsample,\n",
    "        colsample_bytree=best_forward_estimator.colsample_bytree,\n",
    "        reg_alpha=best_forward_estimator.reg_alpha * 1.5,\n",
    "        reg_lambda=best_forward_estimator.reg_lambda * 1.5,\n",
    "        min_child_samples=best_forward_estimator.min_child_samples,\n",
    "        min_split_gain=best_forward_estimator.min_split_gain,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1,\n",
    "    )\n",
    "elif best_forward_estimator is not None and best_model_name == \"random_forest_robust\":\n",
    "    # Use Random Forest with similar parameters\n",
    "    reverse_base_model = RandomForestRegressor(\n",
    "        n_estimators=best_forward_estimator.n_estimators,\n",
    "        max_depth=(\n",
    "            min(best_forward_estimator.max_depth + 2, 15)\n",
    "            if best_forward_estimator.max_depth\n",
    "            else None\n",
    "        ),\n",
    "        min_samples_split=best_forward_estimator.min_samples_split,\n",
    "        min_samples_leaf=best_forward_estimator.min_samples_leaf,\n",
    "        max_features=best_forward_estimator.max_features,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "elif best_forward_estimator is not None and best_model_name == \"mlp_robust\":\n",
    "    # Use MLP with similar parameters but adapted for reverse modeling\n",
    "    reverse_base_model = MLPRegressor(\n",
    "        hidden_layer_sizes=best_forward_estimator.hidden_layer_sizes,\n",
    "        activation=best_forward_estimator.activation,\n",
    "        solver=best_forward_estimator.solver,\n",
    "        alpha=best_forward_estimator.alpha * 1.5,  # Higher regularization\n",
    "        learning_rate_init=best_forward_estimator.learning_rate_init * 0.8,\n",
    "        max_iter=best_forward_estimator.max_iter,\n",
    "        early_stopping=best_forward_estimator.early_stopping,\n",
    "        validation_fraction=best_forward_estimator.validation_fraction,\n",
    "        n_iter_no_change=best_forward_estimator.n_iter_no_change,\n",
    "        random_state=42,\n",
    "    )\n",
    "elif (\n",
    "    best_forward_estimator is not None and best_model_name == \"gradient_boosting_robust\"\n",
    "):\n",
    "    # Use Gradient Boosting with similar parameters\n",
    "    reverse_base_model = GradientBoostingRegressor(\n",
    "        n_estimators=best_forward_estimator.n_estimators,\n",
    "        max_depth=min(best_forward_estimator.max_depth + 2, 10),\n",
    "        learning_rate=max(best_forward_estimator.learning_rate * 0.8, 0.01),\n",
    "        subsample=best_forward_estimator.subsample,\n",
    "        min_samples_split=best_forward_estimator.min_samples_split,\n",
    "        min_samples_leaf=best_forward_estimator.min_samples_leaf,\n",
    "        random_state=42,\n",
    "    )\n",
    "else:\n",
    "    # Fallback to XGBoost if unknown model type or couldn't extract estimator\n",
    "    print(f\"     ⚠️  Using XGBoost fallback for reverse model\")\n",
    "    reverse_base_model = XGBRegressor(\n",
    "        n_estimators=300,\n",
    "        max_depth=8,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.5,\n",
    "        reg_lambda=2.0,\n",
    "        gamma=0.1,\n",
    "        min_child_weight=3,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap in MultiOutputRegressor for multiple target prediction\n",
    "reverse_model = MultiOutputRegressor(reverse_base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultiOutputRegressor(estimator=RandomForestRegressor(max_depth=14,\n",
       "                                                     max_features=&#x27;sqrt&#x27;,\n",
       "                                                     min_samples_leaf=5,\n",
       "                                                     min_samples_split=10,\n",
       "                                                     n_estimators=200,\n",
       "                                                     n_jobs=-1,\n",
       "                                                     random_state=42))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>MultiOutputRegressor</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.multioutput.MultiOutputRegressor.html\">?<span>Documentation for MultiOutputRegressor</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>MultiOutputRegressor(estimator=RandomForestRegressor(max_depth=14,\n",
       "                                                     max_features=&#x27;sqrt&#x27;,\n",
       "                                                     min_samples_leaf=5,\n",
       "                                                     min_samples_split=10,\n",
       "                                                     n_estimators=200,\n",
       "                                                     n_jobs=-1,\n",
       "                                                     random_state=42))</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>estimator: RandomForestRegressor</div></div></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestRegressor(max_depth=14, max_features=&#x27;sqrt&#x27;, min_samples_leaf=5,\n",
       "                      min_samples_split=10, n_estimators=200, n_jobs=-1,\n",
       "                      random_state=42)</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>RandomForestRegressor</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.ensemble.RandomForestRegressor.html\">?<span>Documentation for RandomForestRegressor</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestRegressor(max_depth=14, max_features=&#x27;sqrt&#x27;, min_samples_leaf=5,\n",
       "                      min_samples_split=10, n_estimators=200, n_jobs=-1,\n",
       "                      random_state=42)</pre></div> </div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultiOutputRegressor(estimator=RandomForestRegressor(max_depth=14,\n",
       "                                                     max_features='sqrt',\n",
       "                                                     min_samples_leaf=5,\n",
       "                                                     min_samples_split=10,\n",
       "                                                     n_estimators=200,\n",
       "                                                     n_jobs=-1,\n",
       "                                                     random_state=42))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train: y_reverse (performance) -> X_reverse (parameters)\n",
    "reverse_model.fit(y_rev_train_scaled, X_rev_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate reverse model\n",
    "X_pred_scaled = reverse_model.predict(y_rev_test_scaled)\n",
    "X_pred = reverse_X_scaler.inverse_transform(X_pred_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_r2 = r2_score(X_rev_test, X_pred)\n",
    "reverse_mae = mean_absolute_error(X_rev_test, X_pred)\n",
    "reverse_rmse = np.sqrt(mean_squared_error(X_rev_test, X_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Reverse model trained using Random Forest Robust (Chained) architecture!\n",
      "R² = 0.8001\n",
      "MAE = 20.5102\n",
      "RMSE = 70.0006\n",
      "Architecture consistency: Forward ↔ Reverse both use Random Forest Robust (Chained)\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"\\n Reverse model trained using {model_performance[best_model_name]['name']} architecture!\"\n",
    ")\n",
    "print(f\"R² = {reverse_r2:.4f}\")\n",
    "print(f\"MAE = {reverse_mae:.4f}\")\n",
    "print(f\"RMSE = {reverse_rmse:.4f}\")\n",
    "print(\n",
    "    f\"Architecture consistency: Forward ↔ Reverse both use {model_performance[best_model_name]['name']}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_performance[\"reverse\"] = {\n",
    "    \"r2\": reverse_r2,\n",
    "    \"mae\": reverse_mae,\n",
    "    \"rmse\": reverse_rmse,\n",
    "    \"param_names\": numerical_params,\n",
    "    \"target_names\": target_names,\n",
    "    \"architecture\": model_performance[best_model_name][\"name\"],\n",
    "    \"base_model_type\": best_model_name,\n",
    "    \"architecture_consistency\": f\"Uses same architecture as best forward model ({model_performance[best_model_name]['name']})\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INVERSE DESIGN OPTIMIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_TO_OPTIMIZE = [\"fW\", \"current\", \"ind\", \"Rd\", \"Cs\", \"Rs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_optimize = [\n",
    "    feat for feat in FEATURES_TO_OPTIMIZE if feat in numerical_params\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  fW: [1.00e-07, 1.09e-05]\n",
      "  current: [3.00e-04, 2.70e-03]\n",
      "  ind: [0.00e+00, 3.30e-09]\n",
      "  Rd: [0.00e+00, 1.65e+03]\n",
      "  Cs: [0.00e+00, 1.10e-12]\n",
      "  Rs: [0.00e+00, 1.65e+03]\n"
     ]
    }
   ],
   "source": [
    "# Setup parameter bounds based on data ranges\n",
    "parameter_bounds = {}\n",
    "for feat in features_to_optimize:\n",
    "    min_val = df[feat].min()\n",
    "    max_val = df[feat].max()\n",
    "    # Add some margin to bounds\n",
    "    margin = (max_val - min_val) * 0.1\n",
    "    parameter_bounds[feat] = (max(0, min_val - margin), max_val + margin)\n",
    "    print(\n",
    "        f\"  {feat}: [{parameter_bounds[feat][0]:.2e}, {parameter_bounds[feat][1]:.2e}]\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define realistic attenuation target ranges (based on finalBaseline)\n",
    "attenuation_ranges = {\n",
    "    \"stage 1 3.5G attenuation\": (-30, -6.577),\n",
    "    \"stage 2 3.5G attenuation\": (-30, -6.577),\n",
    "    \"stage 1 7G attenuation\": (-35, -9.067),\n",
    "    \"stage 2 7G attenuation\": (-35, -9.067),\n",
    "    \"stage 1 14G attenuation\": (-40, -13.14),\n",
    "    \"stage 2 14G attenuation\": (-40, -13.14),\n",
    "    \"stage 1 28G attenuation\": (-18.86, 0),\n",
    "    \"stage 2 28G attenuation\": (-18.86, 0),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to only include targets that exist in our model\n",
    "available_attenuation_targets = [\n",
    "    target for target in attenuation_ranges.keys() if target in target_names\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify eye metric targets for multi-objective optimization\n",
    "available_eye_targets = [\n",
    "    target\n",
    "    for target in target_names\n",
    "    if (\"eye_maxHeight\" in target or \"eye_maxWidth\" in target)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create constant features template (median values for non-optimized features)\n",
    "constant_features_template = {}\n",
    "for feat in feature_names:\n",
    "    if feat in features_to_optimize:\n",
    "        constant_features_template[feat] = df[\n",
    "            feat\n",
    "        ].median()  # Will be overridden during optimization\n",
    "    elif feat in [\"Stage 1 Region\", \"Hard Constraint of neg2 on 0.1G Status\"]:\n",
    "        # Set categorical features to most common values\n",
    "        constant_features_template[feat] = (\n",
    "            df[feat].mode()[0] if not df[feat].empty else 0\n",
    "        )\n",
    "    else:\n",
    "        constant_features_template[feat] = df[feat].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define multi-objective optimization function\n",
    "def objective_function(\n",
    "    x,\n",
    "    target_attenuations,\n",
    "    target_indices,\n",
    "    eye_indices,\n",
    "    attenuation_weight=0.7,\n",
    "    eye_weight=0.3,\n",
    "):\n",
    "    \"\"\"\n",
    "    Multi-objective optimization function for inverse design\n",
    "\n",
    "    Objectives:\n",
    "    1. Minimize error for specified attenuation targets (minimize)\n",
    "    2. Maximize eye_maxHeight and eye_maxWidth metrics (maximize)\n",
    "\n",
    "    Args:\n",
    "        x: Array of parameter values to optimize\n",
    "        target_attenuations: Desired attenuation values\n",
    "        target_indices: Indices of attenuation targets in the full target array\n",
    "        eye_indices: Indices of eye metrics in the full target array\n",
    "        attenuation_weight: Weight for attenuation error minimization (0-1)\n",
    "        eye_weight: Weight for eye metrics maximization (0-1)\n",
    "\n",
    "    Returns:\n",
    "        Composite score (lower is better)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create input DataFrame with optimized parameters\n",
    "        input_params = constant_features_template.copy()\n",
    "        for i, feat in enumerate(features_to_optimize):\n",
    "            input_params[feat] = x[i]\n",
    "\n",
    "        # Convert to DataFrame and ensure correct order\n",
    "        input_df = pd.DataFrame([input_params])\n",
    "        input_df = input_df[feature_names]\n",
    "\n",
    "        # Preprocess\n",
    "        input_processed = preprocessor.transform(input_df)\n",
    "\n",
    "        # Predict all targets using the best model\n",
    "        all_predictions = best_model.predict(input_processed).flatten()\n",
    "\n",
    "        # Objective 1: Minimize attenuation error (MSE)\n",
    "        attenuation_predictions = all_predictions[target_indices]\n",
    "        attenuation_mse = np.mean((attenuation_predictions - target_attenuations) ** 2)\n",
    "\n",
    "        # Normalize attenuation error (typical range: 0-100)\n",
    "        normalized_attenuation_error = attenuation_mse / 100.0\n",
    "\n",
    "        # Objective 2: Maximize eye metrics (convert to minimization problem)\n",
    "        if len(eye_indices) > 0:\n",
    "            eye_predictions = all_predictions[eye_indices]\n",
    "            # For maximization, we minimize the negative sum\n",
    "            # Normalize eye metrics (typical range: 0-1 for eye metrics)\n",
    "            normalized_eye_sum = np.sum(np.maximum(eye_predictions, 0)) / len(\n",
    "                eye_indices\n",
    "            )\n",
    "            # Convert to minimization: higher eye values = lower cost\n",
    "            eye_cost = 1.0 - np.tanh(normalized_eye_sum)  # tanh to bound between 0-1\n",
    "        else:\n",
    "            eye_cost = 0.0\n",
    "\n",
    "        # Composite objective (weighted sum)\n",
    "        composite_score = (\n",
    "            attenuation_weight * normalized_attenuation_error + eye_weight * eye_cost\n",
    "        )\n",
    "\n",
    "        return composite_score\n",
    "\n",
    "    except Exception as e:\n",
    "        # Return large penalty for invalid parameters\n",
    "        return 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_optimization_tests = 50  # Number of random optimization tests\n",
    "optimization_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oystu\\AppData\\Local\\Temp\\ipykernel_7280\\2578146115.py:42: OptimizeWarning: Unknown solver options: maxiter\n",
      "  optimization_result = minimize(\n",
      "C:\\Users\\oystu\\AppData\\Local\\Temp\\ipykernel_7280\\2578146115.py:42: OptimizeWarning: Unknown solver options: maxiter\n",
      "  optimization_result = minimize(\n",
      "C:\\Users\\oystu\\AppData\\Local\\Temp\\ipykernel_7280\\2578146115.py:42: OptimizeWarning: Unknown solver options: maxiter\n",
      "  optimization_result = minimize(\n",
      "C:\\Users\\oystu\\AppData\\Local\\Temp\\ipykernel_7280\\2578146115.py:42: OptimizeWarning: Unknown solver options: maxiter\n",
      "  optimization_result = minimize(\n",
      "C:\\Users\\oystu\\AppData\\Local\\Temp\\ipykernel_7280\\2578146115.py:42: OptimizeWarning: Unknown solver options: maxiter\n",
      "  optimization_result = minimize(\n",
      "C:\\Users\\oystu\\AppData\\Local\\Temp\\ipykernel_7280\\2578146115.py:42: OptimizeWarning: Unknown solver options: maxiter\n",
      "  optimization_result = minimize(\n",
      "C:\\Users\\oystu\\AppData\\Local\\Temp\\ipykernel_7280\\2578146115.py:42: OptimizeWarning: Unknown solver options: maxiter\n",
      "  optimization_result = minimize(\n",
      "C:\\Users\\oystu\\AppData\\Local\\Temp\\ipykernel_7280\\2578146115.py:42: OptimizeWarning: Unknown solver options: maxiter\n",
      "  optimization_result = minimize(\n",
      "C:\\Users\\oystu\\AppData\\Local\\Temp\\ipykernel_7280\\2578146115.py:42: OptimizeWarning: Unknown solver options: maxiter\n",
      "  optimization_result = minimize(\n",
      "C:\\Users\\oystu\\AppData\\Local\\Temp\\ipykernel_7280\\2578146115.py:42: OptimizeWarning: Unknown solver options: maxiter\n",
      "  optimization_result = minimize(\n",
      "C:\\Users\\oystu\\AppData\\Local\\Temp\\ipykernel_7280\\2578146115.py:42: OptimizeWarning: Unknown solver options: maxiter\n",
      "  optimization_result = minimize(\n",
      "C:\\Users\\oystu\\AppData\\Local\\Temp\\ipykernel_7280\\2578146115.py:42: OptimizeWarning: Unknown solver options: maxiter\n",
      "  optimization_result = minimize(\n",
      "C:\\Users\\oystu\\AppData\\Local\\Temp\\ipykernel_7280\\2578146115.py:42: OptimizeWarning: Unknown solver options: maxiter\n",
      "  optimization_result = minimize(\n",
      "C:\\Users\\oystu\\AppData\\Local\\Temp\\ipykernel_7280\\2578146115.py:42: OptimizeWarning: Unknown solver options: maxiter\n",
      "  optimization_result = minimize(\n",
      "C:\\Users\\oystu\\AppData\\Local\\Temp\\ipykernel_7280\\2578146115.py:42: OptimizeWarning: Unknown solver options: maxiter\n",
      "  optimization_result = minimize(\n",
      "C:\\Users\\oystu\\AppData\\Local\\Temp\\ipykernel_7280\\2578146115.py:42: OptimizeWarning: Unknown solver options: maxiter\n",
      "  optimization_result = minimize(\n",
      "C:\\Users\\oystu\\AppData\\Local\\Temp\\ipykernel_7280\\2578146115.py:42: OptimizeWarning: Unknown solver options: maxiter\n",
      "  optimization_result = minimize(\n"
     ]
    }
   ],
   "source": [
    "for test_idx in range(num_optimization_tests):\n",
    "    # Use different random seed for each test to ensure diversity\n",
    "    np.random.seed(42 + test_idx * 7)  # Different seed for each test\n",
    "\n",
    "    # Generate random target attenuations within realistic ranges\n",
    "    desired_attenuations = {}\n",
    "    target_attenuations_array = []\n",
    "    target_indices = []\n",
    "    eye_indices = []\n",
    "\n",
    "    for target in available_attenuation_targets:\n",
    "        if target in attenuation_ranges:\n",
    "            min_atten, max_atten = attenuation_ranges[target]\n",
    "            desired_value = np.random.uniform(min_atten, max_atten)\n",
    "            desired_attenuations[target] = desired_value\n",
    "            target_attenuations_array.append(desired_value)\n",
    "            target_indices.append(target_names.index(target))\n",
    "\n",
    "    # Get eye metric indices for multi-objective optimization\n",
    "    for target in available_eye_targets:\n",
    "        eye_indices.append(target_names.index(target))\n",
    "\n",
    "    target_attenuations_array = np.array(target_attenuations_array)\n",
    "\n",
    "    # DIVERSE INITIAL GUESSES - Use random starting points instead of median\n",
    "    initial_guess = []\n",
    "    for feat in features_to_optimize:\n",
    "        min_bound, max_bound = parameter_bounds[feat]\n",
    "        # Generate random initial guess within bounds\n",
    "        random_start = np.random.uniform(min_bound, max_bound)\n",
    "        initial_guess.append(random_start)\n",
    "\n",
    "    # Setup bounds for scipy optimization\n",
    "    scipy_bounds = [parameter_bounds[feat] for feat in features_to_optimize]\n",
    "\n",
    "    # Try multiple optimization methods for diversity\n",
    "    optimization_methods = [\"L-BFGS-B\", \"TNC\", \"SLSQP\"]\n",
    "    method_choice = optimization_methods[test_idx % len(optimization_methods)]\n",
    "\n",
    "    try:\n",
    "        # Run multi-objective optimization with diverse methods\n",
    "        optimization_result = minimize(\n",
    "            fun=objective_function,\n",
    "            x0=initial_guess,\n",
    "            args=(target_attenuations_array, target_indices, eye_indices),\n",
    "            method=method_choice,\n",
    "            bounds=scipy_bounds,\n",
    "            options={\"maxiter\": 300, \"ftol\": 1e-8},\n",
    "        )\n",
    "\n",
    "        if optimization_result.success:\n",
    "            # Get optimized parameters\n",
    "            optimized_params = {}\n",
    "            for i, feat in enumerate(features_to_optimize):\n",
    "                optimized_params[feat] = optimization_result.x[i]\n",
    "\n",
    "            # ADD CONTROLLED DIVERSITY: Add small random perturbations to avoid identical solutions\n",
    "            # This ensures we get diverse parameter combinations for testing\n",
    "            diversity_factor = 0.05  # 5% variation\n",
    "            for feat in features_to_optimize:\n",
    "                min_bound, max_bound = parameter_bounds[feat]\n",
    "                param_range = max_bound - min_bound\n",
    "                # Add random perturbation within 5% of parameter range\n",
    "                perturbation = (\n",
    "                    np.random.uniform(-diversity_factor, diversity_factor) * param_range\n",
    "                )\n",
    "                perturbed_value = optimized_params[feat] + perturbation\n",
    "                # Ensure still within bounds\n",
    "                optimized_params[feat] = np.clip(perturbed_value, min_bound, max_bound)\n",
    "\n",
    "            # Validate the result by forward prediction\n",
    "            input_params = constant_features_template.copy()\n",
    "            input_params.update(optimized_params)\n",
    "\n",
    "            input_df = pd.DataFrame([input_params])\n",
    "            input_df = input_df[feature_names]\n",
    "            input_processed = preprocessor.transform(input_df)\n",
    "            final_predictions = best_model.predict(input_processed).flatten()\n",
    "\n",
    "            # Extract attenuation predictions\n",
    "            final_attenuation_predictions = final_predictions[target_indices]\n",
    "\n",
    "            # Calculate errors for attenuation targets\n",
    "            individual_errors = np.abs(\n",
    "                final_attenuation_predictions - target_attenuations_array\n",
    "            )\n",
    "            mse_error = np.mean(\n",
    "                (final_attenuation_predictions - target_attenuations_array) ** 2\n",
    "            )\n",
    "            mae_error = np.mean(individual_errors)\n",
    "            max_error = np.max(individual_errors)\n",
    "\n",
    "            # Calculate eye metrics performance\n",
    "            eye_performance = {}\n",
    "            if len(eye_indices) > 0:\n",
    "                eye_predictions = final_predictions[eye_indices]\n",
    "                eye_performance = {\n",
    "                    \"eye_metrics_sum\": np.sum(eye_predictions),\n",
    "                    \"eye_metrics_avg\": np.mean(eye_predictions),\n",
    "                    \"eye_metrics_min\": np.min(eye_predictions),\n",
    "                    \"eye_metrics_max\": np.max(eye_predictions),\n",
    "                }\n",
    "\n",
    "            # Store detailed results\n",
    "            result_entry = {\n",
    "                \"test_id\": test_idx + 1,\n",
    "                \"success\": True,\n",
    "                \"mse_error\": mse_error,\n",
    "                \"mae_error\": mae_error,\n",
    "                \"max_error\": max_error,\n",
    "                \"optimization_iterations\": optimization_result.nit,\n",
    "                \"optimization_function_calls\": optimization_result.nfev,\n",
    "            }\n",
    "\n",
    "            # Add eye metrics performance\n",
    "            result_entry.update(eye_performance)\n",
    "\n",
    "            # Add desired targets\n",
    "            for target, value in desired_attenuations.items():\n",
    "                result_entry[f\"desired_{target}\"] = value\n",
    "\n",
    "            # Add predicted targets\n",
    "            for i, target in enumerate(available_attenuation_targets):\n",
    "                result_entry[f\"predicted_{target}\"] = final_attenuation_predictions[i]\n",
    "\n",
    "            # Add optimized parameters\n",
    "            for feat, value in optimized_params.items():\n",
    "                result_entry[feat] = value\n",
    "\n",
    "            # Add constant parameters\n",
    "            for feat, value in constant_features_template.items():\n",
    "                if feat not in optimized_params:\n",
    "                    result_entry[f\"constant_{feat}\"] = value\n",
    "\n",
    "            optimization_results.append(result_entry)\n",
    "\n",
    "        else:\n",
    "            result_entry = {\n",
    "                \"test_id\": test_idx + 1,\n",
    "                \"success\": False,\n",
    "                \"failure_reason\": optimization_result.message,\n",
    "                \"mse_error\": np.nan,\n",
    "                \"mae_error\": np.nan,\n",
    "                \"max_error\": np.nan,\n",
    "            }\n",
    "\n",
    "            # Add desired targets\n",
    "            for target, value in desired_attenuations.items():\n",
    "                result_entry[f\"desired_{target}\"] = value\n",
    "\n",
    "            optimization_results.append(result_entry)\n",
    "\n",
    "    except Exception as e:\n",
    "        result_entry = {\n",
    "            \"test_id\": test_idx + 1,\n",
    "            \"success\": False,\n",
    "            \"failure_reason\": str(e),\n",
    "            \"mse_error\": np.nan,\n",
    "            \"mae_error\": np.nan,\n",
    "            \"max_error\": np.nan,\n",
    "        }\n",
    "\n",
    "        # Add desired targets\n",
    "        for target, value in desired_attenuations.items():\n",
    "            result_entry[f\"desired_{target}\"] = value\n",
    "\n",
    "        optimization_results.append(result_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "successful_optimizations = [r for r in optimization_results if r[\"success\"]]\n",
    "success_rate = len(successful_optimizations) / len(optimization_results) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inverse Design Optimization Summary:\n",
      "Total tests: 50\n",
      "Successful optimizations: 34\n",
      "Success rate: 68.0%\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nInverse Design Optimization Summary:\")\n",
    "print(f\"Total tests: {len(optimization_results)}\")\n",
    "print(f\"Successful optimizations: {len(successful_optimizations)}\")\n",
    "print(f\"Success rate: {success_rate:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MSE: 475.169794\n",
      "Average MAE: 19.748 dB\n",
      "Average Max Error: 32.384 dB\n",
      "Best MSE: 194.358825\n",
      "Best MAE: 13.088 dB\n"
     ]
    }
   ],
   "source": [
    "if successful_optimizations:\n",
    "    mse_errors = [r[\"mse_error\"] for r in successful_optimizations]\n",
    "    mae_errors = [r[\"mae_error\"] for r in successful_optimizations]\n",
    "    max_errors = [r[\"max_error\"] for r in successful_optimizations]\n",
    "\n",
    "    print(f\"Average MSE: {np.mean(mse_errors):.6f}\")\n",
    "    print(f\"Average MAE: {np.mean(mae_errors):.3f} dB\")\n",
    "    print(f\"Average Max Error: {np.mean(max_errors):.3f} dB\")\n",
    "    print(f\"Best MSE: {np.min(mse_errors):.6f}\")\n",
    "    print(f\"Best MAE: {np.min(mae_errors):.3f} dB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_random_tests = 30  # Additional random combinations\n",
    "random_test_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_idx in range(num_random_tests):\n",
    "    # Use different random seed for each random test\n",
    "    np.random.seed(1000 + test_idx * 13)\n",
    "\n",
    "    # Generate completely random parameters within bounds\n",
    "    random_params = {}\n",
    "    for feat in features_to_optimize:\n",
    "        min_bound, max_bound = parameter_bounds[feat]\n",
    "        random_params[feat] = np.random.uniform(min_bound, max_bound)\n",
    "\n",
    "    # Create full parameter set\n",
    "    full_params = constant_features_template.copy()\n",
    "    full_params.update(random_params)\n",
    "\n",
    "    # Forward prediction to get performance\n",
    "    input_df = pd.DataFrame([full_params])\n",
    "    input_df = input_df[feature_names]\n",
    "    input_processed = preprocessor.transform(input_df)\n",
    "    predicted_performance = best_model.predict(input_processed).flatten()\n",
    "\n",
    "    # Store random test result\n",
    "    random_result = {\n",
    "        \"test_id\": f\"random_{test_idx + 1}\",\n",
    "        \"test_type\": \"random_generation\",\n",
    "        \"success\": True,\n",
    "    }\n",
    "\n",
    "    # Add random parameters\n",
    "    for feat, value in random_params.items():\n",
    "        random_result[feat] = value\n",
    "\n",
    "    # Add constant parameters\n",
    "    for feat, value in constant_features_template.items():\n",
    "        if feat not in random_params:\n",
    "            random_result[f\"constant_{feat}\"] = value\n",
    "\n",
    "    # Add predicted performance for all targets\n",
    "    for i, target in enumerate(target_names):\n",
    "        random_result[f\"predicted_{target}\"] = predicted_performance[i]\n",
    "\n",
    "    # Add some synthetic \"desired\" values for consistency\n",
    "    for target in available_attenuation_targets:\n",
    "        if target in attenuation_ranges:\n",
    "            min_atten, max_atten = attenuation_ranges[target]\n",
    "            random_result[f\"desired_{target}\"] = np.random.uniform(min_atten, max_atten)\n",
    "\n",
    "    random_test_results.append(random_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine optimization results with random results\n",
    "all_test_results = optimization_results + random_test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined results saved to: ctle_circuit_generation_with_DNN_models_V2/inverse_optimization_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Save combined results\n",
    "optimization_results_df = pd.DataFrame(all_test_results)\n",
    "optimization_results_file = f\"{save_directory}/inverse_optimization_results.csv\"\n",
    "optimization_results_df.to_csv(optimization_results_file, index=False)\n",
    "print(f\"Combined results saved to: {optimization_results_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diverse parameters only: ctle_circuit_generation_with_DNN_models_V2/diverse_parameter_combinations.csv\n"
     ]
    }
   ],
   "source": [
    "# Create a separate file with ONLY the diverse parameter combinations for easy testing\n",
    "diverse_params_data = []\n",
    "for result in all_test_results:\n",
    "    if result.get(\"success\", False):\n",
    "        param_entry = {\"test_id\": result.get(\"test_id\", \"unknown\")}\n",
    "\n",
    "        # Add the optimized/random parameters\n",
    "        for feat in features_to_optimize:\n",
    "            if feat in result:\n",
    "                param_entry[feat] = result[feat]\n",
    "\n",
    "        # Add some constant parameters for completeness\n",
    "        for feat in [\"Stage 1 Region\", \"Hard Constraint of neg2 on 0.1G Status\"]:\n",
    "            if f\"constant_{feat}\" in result:\n",
    "                param_entry[feat] = result[f\"constant_{feat}\"]\n",
    "            elif feat in constant_features_template:\n",
    "                param_entry[feat] = constant_features_template[feat]\n",
    "\n",
    "        diverse_params_data.append(param_entry)\n",
    "\n",
    "diverse_params_df = pd.DataFrame(diverse_params_data)\n",
    "diverse_params_file = f\"{save_directory}/diverse_parameter_combinations.csv\"\n",
    "diverse_params_df.to_csv(diverse_params_file, index=False)\n",
    "print(f\"Diverse parameters only: {diverse_params_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE COMPLETE SYSTEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(save_directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models\n",
    "for name, model in models.items():\n",
    "    joblib.dump(model, f\"{save_directory}/forward_model_{name}.joblib\")\n",
    "\n",
    "# Save preprocessor\n",
    "joblib.dump(preprocessor, f\"{save_directory}/preprocessor.joblib\")\n",
    "\n",
    "# Save reverse model components\n",
    "joblib.dump(reverse_model, f\"{save_directory}/reverse_model.joblib\")\n",
    "joblib.dump(reverse_X_scaler, f\"{save_directory}/reverse_x_scaler.joblib\")\n",
    "joblib.dump(reverse_y_scaler, f\"{save_directory}/reverse_y_scaler.joblib\")\n",
    "\n",
    "# Save comprehensive metadata\n",
    "metadata = {\n",
    "    \"dataset_shape\": df.shape,\n",
    "    \"target_categories\": {\n",
    "        \"good_targets\": good_targets,\n",
    "        \"sparse_targets\": sparse_targets,\n",
    "        \"constant_targets\": constant_targets,\n",
    "        \"problematic_targets\": problematic_targets,\n",
    "    },\n",
    "    \"target_preprocessing_info\": target_preprocessing_info,\n",
    "    \"model_performance\": model_performance,\n",
    "    \"model_scores\": model_scores,\n",
    "    \"best_model_name\": best_model_name,\n",
    "    \"best_composite_score\": best_composite_score,\n",
    "    \"feature_names\": feature_names,\n",
    "    \"target_names\": target_names,\n",
    "    \"numerical_params\": numerical_params,\n",
    "    \"total_targets_used\": len(target_names),\n",
    "    \"optimization_results\": {\n",
    "        \"optimization_tests\": len(optimization_results),\n",
    "        \"random_tests\": (\n",
    "            len(random_test_results) if \"random_test_results\" in locals() else 0\n",
    "        ),\n",
    "        \"total_tests\": (\n",
    "            len(all_test_results)\n",
    "            if \"all_test_results\" in locals()\n",
    "            else len(optimization_results)\n",
    "        ),\n",
    "        \"successful_tests\": len(successful_optimizations),\n",
    "        \"success_rate\": success_rate,\n",
    "        \"features_optimized\": features_to_optimize,\n",
    "        \"parameter_bounds\": parameter_bounds,\n",
    "        \"attenuation_targets\": available_attenuation_targets,\n",
    "        \"average_mae\": np.mean(mae_errors) if successful_optimizations else None,\n",
    "        \"best_mae\": np.min(mae_errors) if successful_optimizations else None,\n",
    "        \"diversity_enhancements\": [\n",
    "            \"Random initial guesses for each optimization\",\n",
    "            \"Multiple optimization methods (L-BFGS-B, TNC, SLSQP)\",\n",
    "            \"Controlled parameter perturbations (±5%)\",\n",
    "            \"Additional random parameter generation\",\n",
    "            \"Different random seeds for each test\",\n",
    "        ],\n",
    "    },\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"system_version\": \"2.1_with_inverse_optimization\",\n",
    "}\n",
    "\n",
    "with open(f\"{save_directory}/metadata.json\", \"w\") as f:\n",
    "    json.dump(metadata, f, indent=2, default=str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
